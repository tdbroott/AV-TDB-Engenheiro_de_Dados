{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Avaliação Técnica – Engenheiro de Dados\n",
        "**Autor:** Thales Dias Braga\n",
        "\n",
        "Esta avaliação consiste na implementação de um pipeline de dados utilizando **Databricks**, focado no processamento de dados estatísticos (populacionais) de arquivos XML para uma arquitetura de Lakehouse.\n",
        "\n",
        "\n",
        "## Descrição da Solução\n",
        "\n",
        "O pipeline foi desenhado seguindo a **Arquitetura Medalhão**, processando os dados em camadas para garantir qualidade e performance:\n",
        "\n",
        "1.  **Configuração e Setup:** Definição de caminhos para armazenamento utilizando Databricks Volumes e limpeza de ambientes para re-execuções.\n",
        "2.  **Ingestão (Camada Bronze):**\n",
        "    *   Leitura do arquivo fonte em formato XML (`all.xml`).\n",
        "    *   Transformação imediata dos dados brutos para o formato **Delta**.\n",
        "    *   Adição de metadados de ingestão (data de processamento e arquivo de origem).\n",
        "\n",
        "O objetivo principal desta etapa inicial é converter o dado semi-estruturado (XML) para um formato otimizado (Delta) que permita consultas rápidas e manipulação nas etapas subsequentes (Silver e Gold).\n",
        "\n",
        "## Instruções Básicas de Execução\n",
        "\n",
        "### Pré-requisitos\n",
        "*   Acesso a um ambiente **Databricks** com Unity Catalog habilitado (devido ao uso de `/Volumes`).\n",
        "*   Cluster ativo para execução de notebooks Spark/Python.\n",
        "\n",
        "### Configuração do Arquivo de Entrada\n",
        "Certifique-se de que o arquivo de dados `all.xml` esteja carregado no seguinte caminho do Volume (ou ajuste a variável `input_path` no notebook):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Executando o Pipeline\n",
        "1.  Importe o notebook para o seu Workspace do Databricks.\n",
        "2.  Verifique se os caminhos definidos na célula de configuração (`path_bronze`, `path_silver`, `path_gold`) existem ou podem ser criados pelo usuário que executa o script.\n",
        "3.  Execute o notebook selecionando a opção **\"Run All\"**.\n",
        "\n",
        "> **Nota:** O script está configurado para **apagar** os dados existentes nas pastas de destino a cada execução (`dbutils.fs.rm`), ideal para testes e desenvolvimento. ***Para produção, esta lógica deve ser alterada.***\n",
        "\n",
        "##  Decisões Adotadas\n",
        "\n",
        "Abaixo estão as justificativas para as principais decisões técnicas observadas no código:\n",
        "\n",
        "*   **Arquitetura Medalhão (Bronze/Silver/Gold):**\n",
        "    *   A separação em camadas (`path_bronze`, `path_silver`, `path_gold`) foi adotada para organizar o fluxo de dados, separando o dado bruto (Bronze) do dado tratado e do dado agregado para negócios.\n",
        "\n",
        "*   **Uso de Databricks Volumes:**\n",
        "    *   A utilização de caminhos iniciados por `/Volumes/...` indica o uso do Unity Catalog, garantindo governança moderna e facilidade de acesso aos arquivos como se fossem um sistema de arquivos local.\n",
        "\n",
        "*   **Conversão XML para Delta:**\n",
        "    *   A decisão de converter o XML para Delta logo na camada Bronze visa **performance**. Dados de arquivos XML são custosos para leitura e não suportam processamento paralelo eficiente. O formato Delta permite transações ACID, compactação e leitura otimizada para as etapas seguintes.\n",
        "\n",
        "*   **Limpeza Automática:**\n",
        "    *   O comando de limpeza no início do script garante que o ambiente esteja \"limpo\" para validar a lógica de ponta a ponta sem duplicidade de dados durante a fase de avaliação técnica.\n"
      ],
      "metadata": {
        "id": "0sfE-Yl4OrOb"
      }
    }
  ]
}